{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n# <font color=\"green\"><u> II. Importing/Loading & checking the data:</u></font>","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install bertopic\n!pip install minisom","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:06:19.037696Z","iopub.execute_input":"2023-05-08T18:06:19.038249Z","iopub.status.idle":"2023-05-08T18:07:38.285792Z","shell.execute_reply.started":"2023-05-08T18:06:19.038216Z","shell.execute_reply":"2023-05-08T18:07:38.284381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True) ","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:09:08.974631Z","iopub.execute_input":"2023-05-08T18:09:08.975004Z","iopub.status.idle":"2023-05-08T18:09:09.089146Z","shell.execute_reply.started":"2023-05-08T18:09:08.97497Z","shell.execute_reply":"2023-05-08T18:09:09.088122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd                                        #Data processing, CSV files I/O (e.g. pd.read_csv)\nimport numpy as np                                         #Linear Algebra: Matrices ...\nimport matplotlib.pyplot as plt                            #Data Visualisation\nimport seaborn as sns    \nfrom bertopic import BERTopic\n\nfrom tqdm import tqdm\n# I discoverd that it's possible to download models for the specific purpose to preprocess scientific texts\n# In the spacy docs I found a specific model for this : https://spacy.io/universe/project/scispacy\n#Downloading en_core_sci_lg model to preprocess abstracts\nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:09:11.064531Z","iopub.execute_input":"2023-05-08T18:09:11.064873Z","iopub.status.idle":"2023-05-08T18:11:27.291648Z","shell.execute_reply.started":"2023-05-08T18:09:11.064846Z","shell.execute_reply":"2023-05-08T18:11:27.290275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import NLP librarys and the spacy package to preprocess the abstract text\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS #import commen list of stopword\nimport en_core_sci_lg  # import downlaoded model\nimport string\nfrom minisom import MiniSom  \nfrom sklearn.cluster import SpectralClustering \nimport scipy.cluster.hierarchy as sch\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:11:27.294364Z","iopub.execute_input":"2023-05-08T18:11:27.295813Z","iopub.status.idle":"2023-05-08T18:11:27.32626Z","shell.execute_reply.started":"2023-05-08T18:11:27.295778Z","shell.execute_reply":"2023-05-08T18:11:27.325372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =  pd.read_csv(\"/kaggle/input/research-papers-dataset/dblp-v10.csv\")\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:11:27.32753Z","iopub.execute_input":"2023-05-08T18:11:27.327959Z","iopub.status.idle":"2023-05-08T18:12:00.307323Z","shell.execute_reply.started":"2023-05-08T18:11:27.327925Z","shell.execute_reply":"2023-05-08T18:12:00.306229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:12:00.309523Z","iopub.execute_input":"2023-05-08T18:12:00.310534Z","iopub.status.idle":"2023-05-08T18:12:01.294664Z","shell.execute_reply.started":"2023-05-08T18:12:00.310501Z","shell.execute_reply":"2023-05-08T18:12:01.293052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:12:01.295839Z","iopub.execute_input":"2023-05-08T18:12:01.296173Z","iopub.status.idle":"2023-05-08T18:12:02.24314Z","shell.execute_reply.started":"2023-05-08T18:12:01.296142Z","shell.execute_reply":"2023-05-08T18:12:02.242317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n# <font color=\"green\"><u> III. Data Cleaning:</u></font>","metadata":{}},{"cell_type":"code","source":"df.dropna(subset='abstract',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:14:32.530242Z","iopub.execute_input":"2023-05-08T18:14:32.53067Z","iopub.status.idle":"2023-05-08T18:14:32.889257Z","shell.execute_reply.started":"2023-05-08T18:14:32.530639Z","shell.execute_reply":"2023-05-08T18:14:32.888318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:14:34.121083Z","iopub.execute_input":"2023-05-08T18:14:34.121828Z","iopub.status.idle":"2023-05-08T18:14:34.939156Z","shell.execute_reply.started":"2023-05-08T18:14:34.121792Z","shell.execute_reply":"2023-05-08T18:14:34.938292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n # <font color=\"green\"><u> IV. NLP data preprocessing:</u></font>","metadata":{"execution":{"iopub.status.busy":"2023-05-08T16:59:57.755922Z","iopub.execute_input":"2023-05-08T16:59:57.756311Z","iopub.status.idle":"2023-05-08T16:59:57.78837Z","shell.execute_reply.started":"2023-05-08T16:59:57.756281Z","shell.execute_reply":"2023-05-08T16:59:57.787068Z"}}},{"cell_type":"code","source":"# Parser\nparser = en_core_sci_lg.load()\nparser.max_length = 7000000 #Limit the size of the parser\n\ndef spacy_tokenizer(sentence):\n    ''' Function to preprocess text of scientific papers \n        (e.g Removing Stopword and puntuations)'''\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] # transform to lowercase and then split the scentence\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ] #remove stopsword an punctuation\n    mytokens = \" \".join([i for i in mytokens]) \n    return mytokens","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:14:38.319339Z","iopub.execute_input":"2023-05-08T18:14:38.319677Z","iopub.status.idle":"2023-05-08T18:14:48.433407Z","shell.execute_reply.started":"2023-05-08T18:14:38.319651Z","shell.execute_reply":"2023-05-08T18:14:48.432221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuations = string.punctuation #list of punctuation to remove from text\nstopwords = list(STOP_WORDS)\nstopwords[:10]","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:14:48.435254Z","iopub.execute_input":"2023-05-08T18:14:48.435937Z","iopub.status.idle":"2023-05-08T18:14:48.443832Z","shell.execute_reply.started":"2023-05-08T18:14:48.435904Z","shell.execute_reply":"2023-05-08T18:14:48.442788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the dataframe contains still hugh amount of data. The process the data faster I reduce the df to 10000 rows\n# The scope of the notebook is not to analyze all data\ndf = df.sample(10000, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:14:48.447047Z","iopub.execute_input":"2023-05-08T18:14:48.447928Z","iopub.status.idle":"2023-05-08T18:14:48.560569Z","shell.execute_reply.started":"2023-05-08T18:14:48.447812Z","shell.execute_reply":"2023-05-08T18:14:48.559498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ndf[\"processed_text\"] = df[\"abstract\"].progress_apply(spacy_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:14:48.562933Z","iopub.execute_input":"2023-05-08T18:14:48.563771Z","iopub.status.idle":"2023-05-08T18:19:28.453055Z","shell.execute_reply.started":"2023-05-08T18:14:48.563737Z","shell.execute_reply":"2023-05-08T18:19:28.45199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n# <font color=\"green\"><u> V. Model Training: BERTopic:</u></font>","metadata":{}},{"cell_type":"markdown","source":"To train our BERTopic model, we make a few adjustments to the default parameters while ensuring originality.\n\nFirstly, we select the embedding model \"paraphrase-MiniLM-L6-v2\" as our preferred choice. This particular embedding model, which can be accessed from the provided link, strikes a balance between performance and speed, making it an excellent option for sentence transformation.\n\nFurthermore, we set the minimum topic size to 50. This parameter determines the smallest allowable size for each topic. By imposing this restriction, we aim to limit the number of generated topics. For instance, if the minimum were set to 10, a significantly larger number of topics would be created, but they might be of lesser significance. In order to prioritize substantial topics, we opt for a minimum size of 10.","metadata":{}},{"cell_type":"code","source":"topic_model = BERTopic(verbose=True, embedding_model=\"paraphrase-MiniLM-L6-v2\", min_topic_size=50)\ntopics, _ = topic_model.fit_transform(df[\"processed_text\"].to_numpy()); len(topic_model.get_topic_info())","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:20:05.530664Z","iopub.execute_input":"2023-05-08T18:20:05.531036Z","iopub.status.idle":"2023-05-08T18:21:05.836381Z","shell.execute_reply.started":"2023-05-08T18:20:05.531007Z","shell.execute_reply":"2023-05-08T18:21:05.83531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n> # <font color=\"green\"><u> A. Topic Representation:</u></font>","metadata":{}},{"cell_type":"code","source":"topic_model.get_topic_info().head(10)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:21:13.338544Z","iopub.execute_input":"2023-05-08T18:21:13.338889Z","iopub.status.idle":"2023-05-08T18:21:13.354352Z","shell.execute_reply.started":"2023-05-08T18:21:13.338861Z","shell.execute_reply":"2023-05-08T18:21:13.353366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.visualize_barchart(top_n_topics=9, height=700)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:21:19.469757Z","iopub.execute_input":"2023-05-08T18:21:19.470171Z","iopub.status.idle":"2023-05-08T18:21:20.55429Z","shell.execute_reply.started":"2023-05-08T18:21:19.470134Z","shell.execute_reply":"2023-05-08T18:21:20.55306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.visualize_term_rank()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:21:30.286006Z","iopub.execute_input":"2023-05-08T18:21:30.286394Z","iopub.status.idle":"2023-05-08T18:21:30.370404Z","shell.execute_reply.started":"2023-05-08T18:21:30.286359Z","shell.execute_reply":"2023-05-08T18:21:30.36882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n> # <font color=\"green\"><u> B. Topic Relationships:</u></font>","metadata":{}},{"cell_type":"code","source":"topic_model.visualize_topics(top_n_topics=21)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:21:35.837807Z","iopub.execute_input":"2023-05-08T18:21:35.838389Z","iopub.status.idle":"2023-05-08T18:21:39.782739Z","shell.execute_reply.started":"2023-05-08T18:21:35.838315Z","shell.execute_reply":"2023-05-08T18:21:39.781895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.visualize_hierarchy(top_n_topics=21, width=800)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:21:48.642692Z","iopub.execute_input":"2023-05-08T18:21:48.643081Z","iopub.status.idle":"2023-05-08T18:21:48.719158Z","shell.execute_reply.started":"2023-05-08T18:21:48.64305Z","shell.execute_reply":"2023-05-08T18:21:48.71809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.visualize_heatmap(n_clusters=5, top_n_topics=21)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:22:08.187658Z","iopub.execute_input":"2023-05-08T18:22:08.188104Z","iopub.status.idle":"2023-05-08T18:22:08.290059Z","shell.execute_reply.started":"2023-05-08T18:22:08.188074Z","shell.execute_reply":"2023-05-08T18:22:08.289052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"8\"></a><br>\n> # <font color=\"green\"><u> C. Topics over Time:</u></font>","metadata":{}},{"cell_type":"code","source":"# year = df.year.astype(np.int64).tolist()\n# year = list(set(year))\n# topics_over_time = topic_model.topics_over_time(df.abstract, topics,year )","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:04:28.336046Z","iopub.status.idle":"2023-05-08T18:04:28.336945Z","shell.execute_reply.started":"2023-05-08T18:04:28.336656Z","shell.execute_reply":"2023-05-08T18:04:28.336682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20, width=900, height=500)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:04:28.338411Z","iopub.status.idle":"2023-05-08T18:04:28.340202Z","shell.execute_reply.started":"2023-05-08T18:04:28.339929Z","shell.execute_reply":"2023-05-08T18:04:28.339957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n# <font color=\"green\"><u> V. Clustering using scikit-learn: Uncovering Patterns in Data:</u></font>","metadata":{}},{"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n># <font color=\"green\"><u> A. Vectorization of the abstracts and dimensionality reduction with PCA:</u></font>\n ","metadata":{}},{"cell_type":"code","source":"# define vec function\ndef vectorize(text, maxx_features):\n    \n    vectorizer = TfidfVectorizer(max_features=maxx_features)\n    X = vectorizer.fit_transform(text)\n    return X","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:22:15.85291Z","iopub.execute_input":"2023-05-08T18:22:15.853258Z","iopub.status.idle":"2023-05-08T18:22:15.858008Z","shell.execute_reply.started":"2023-05-08T18:22:15.853228Z","shell.execute_reply":"2023-05-08T18:22:15.85712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vectorize each processed abstract\ntext = df['processed_text'].values\nX = vectorize(text, 2 ** 12) #arbitrary max feature -_> Hyperpara. for optimisation (?)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:22:17.769945Z","iopub.execute_input":"2023-05-08T18:22:17.770292Z","iopub.status.idle":"2023-05-08T18:22:18.488209Z","shell.execute_reply.started":"2023-05-08T18:22:17.770263Z","shell.execute_reply":"2023-05-08T18:22:18.48735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=0.95, random_state=42) #Keep 95% of the variance\nX_reduced= pca.fit_transform(X.toarray())\nX_reduced.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:22:21.131659Z","iopub.execute_input":"2023-05-08T18:22:21.132089Z","iopub.status.idle":"2023-05-08T18:23:39.424236Z","shell.execute_reply.started":"2023-05-08T18:22:21.132054Z","shell.execute_reply":"2023-05-08T18:23:39.423353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"11\"></a><br>\n># <font color=\"green\"><u> B. Hierarchical clustering:</u></font>","metadata":{}},{"cell_type":"markdown","source":">> ## Dendrogram\nA dendrogram is a diagram representing a tree. This diagrammatic representation is frequently used in different contexts: in hierarchical clustering, it illustrates the arrangement of the clusters produced by the corresponding analyses.","metadata":{}},{"cell_type":"code","source":"plt.figure(1, figsize = (16 ,8))\ndendrogram = sch.dendrogram(sch.linkage(X_reduced, method  = \"ward\"))\n\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distances')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:23:39.426033Z","iopub.execute_input":"2023-05-08T18:23:39.427657Z","iopub.status.idle":"2023-05-08T18:27:34.97915Z","shell.execute_reply.started":"2023-05-08T18:23:39.427619Z","shell.execute_reply":"2023-05-08T18:27:34.978231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"12\"></a><br>\n># <font color=\"green\"><u> C. Spectral clustering:</u></font>","metadata":{}},{"cell_type":"code","source":"# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(X.toarray()) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] ","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:28:34.29509Z","iopub.execute_input":"2023-05-08T18:28:34.295562Z","iopub.status.idle":"2023-05-08T18:28:37.698053Z","shell.execute_reply.started":"2023-05-08T18:28:34.295525Z","shell.execute_reply":"2023-05-08T18:28:37.696827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the clustering model \nspectral_model_rbf = SpectralClustering(n_clusters = 2, affinity ='rbf') \n  \n# Training the model and Storing the predicted cluster labels \nlabels_rbf = spectral_model_rbf.fit_predict(X_principal)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:28:39.171232Z","iopub.execute_input":"2023-05-08T18:28:39.171935Z","iopub.status.idle":"2023-05-08T18:28:59.630216Z","shell.execute_reply.started":"2023-05-08T18:28:39.171899Z","shell.execute_reply":"2023-05-08T18:28:59.629501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing the clustering \nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = SpectralClustering(n_clusters = 2, affinity ='rbf') .fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:29:04.065602Z","iopub.execute_input":"2023-05-08T18:29:04.065968Z","iopub.status.idle":"2023-05-08T18:29:27.217488Z","shell.execute_reply.started":"2023-05-08T18:29:04.065938Z","shell.execute_reply":"2023-05-08T18:29:27.216403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"13\"></a><br>\n># <font color=\"green\"><u> D. Self-organizing maps (SOM):</u></font>","metadata":{}},{"cell_type":"code","source":"X_principal = X_principal.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:29:41.713674Z","iopub.execute_input":"2023-05-08T18:29:41.714028Z","iopub.status.idle":"2023-05-08T18:29:41.718255Z","shell.execute_reply.started":"2023-05-08T18:29:41.713999Z","shell.execute_reply":"2023-05-08T18:29:41.717401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I use linear som topography\nsom_shape = (1, 5)\n\nsom = MiniSom(som_shape[0], som_shape[1], X_principal.shape[1], sigma=0.5, learning_rate=0.5)\n\nmax_iter = 1000\nq_error = []\nt_error = []\n\nfor i in range(max_iter):\n    rand_i = np.random.randint(len(X_principal))\n    som.update(X_principal[rand_i], som.winner(X_principal[rand_i]), i, max_iter)\n    q_error.append(som.quantization_error(X_principal))\n    t_error.append(som.topographic_error(X_principal))\n\nplt.plot(np.arange(max_iter), q_error, label='quantization error')\nplt.plot(np.arange(max_iter), t_error, label='topographic error')\nplt.ylabel('Quantization error')\nplt.xlabel('Iteration index')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:29:47.703215Z","iopub.execute_input":"2023-05-08T18:29:47.703606Z","iopub.status.idle":"2023-05-08T18:29:53.469177Z","shell.execute_reply.started":"2023-05-08T18:29:47.703576Z","shell.execute_reply":"2023-05-08T18:29:53.468259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# each neuron represents a cluster\nwinner_coordinates = np.array([som.winner(x) for x in X_principal]).T\n\n# with np.ravel_multi_index we convert the bidimensional\n# coordinates to a monodimensional index\ncluster_index = np.ravel_multi_index(winner_coordinates, som_shape)\n\n# Plotting the clusters \nplt.figure(figsize=(10,8))\n\nfor c in np.unique(cluster_index):\n    plt.scatter(X_principal[cluster_index == c, 0],\n                X_principal[cluster_index == c, 1], label='cluster='+str(c), alpha=.7)\n\n# Plotting centroids\nfor centroid in som.get_weights():\n    plt.scatter(centroid[:, 0], centroid[:, 1], marker='x', \n                s=10, linewidths=20, color='k') # label='centroid'\n    \nplt.title(\"Clusters of Customers\")\nplt.xlabel(\"P1\")\nplt.ylabel(\"P2\")\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2023-05-08T18:29:58.317432Z","iopub.execute_input":"2023-05-08T18:29:58.31779Z","iopub.status.idle":"2023-05-08T18:29:59.019235Z","shell.execute_reply.started":"2023-05-08T18:29:58.317761Z","shell.execute_reply":"2023-05-08T18:29:59.018377Z"},"trusted":true},"execution_count":null,"outputs":[]}]}